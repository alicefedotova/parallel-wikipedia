{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/f0Vjx+v86tteiNsA8NlU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ffedox/pbr/blob/main/corpus_creation_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting a domain-specific parallel corpus from Wikipedia"
      ],
      "metadata": {
        "id": "GpNfRNZlXUtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "3cNlMWd_XYgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Wikipedia API"
      ],
      "metadata": {
        "id": "czRX_YxkXoEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Wikipedia-API](https://github.com/martin-majlis/Wikipedia-API) is a Python wrapper for Wikipedias' API. It supports extracting texts, sections, links, categories, translations, etc from Wikipedia."
      ],
      "metadata": {
        "id": "s0be9vWTXcrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QvrwAFtyXUZY"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia-api --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Sentence Transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "SGYAHuzoXm8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Sentence Transformers](https://www.sbert.net/) is a Python framework for state-of-the-art sentence, text and image embeddings. "
      ],
      "metadata": {
        "id": "1jfnjNU7Xv0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers --quiet"
      ],
      "metadata": {
        "id": "c6CK2cyDXwKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Imports"
      ],
      "metadata": {
        "id": "0-Zy9qgqYWkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "from json import JSONDecodeError\n",
        "\n",
        "import wikipediaapi\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "gEY0JugNYjpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extracting an IT-EN comparable corpus from Category: pages"
      ],
      "metadata": {
        "id": "REpfd_uhY5lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will extract a comparable corpus by scraping Category: pages in Italian (it is more likely to find correspondences from IT to EN than the other way around)."
      ],
      "metadata": {
        "id": "2--kOfviZIJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each page linked in the IT Category: page, we will be retrieving the EN equivalent."
      ],
      "metadata": {
        "id": "OPKtgZNWZYn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Here, we will limit the search to the summaries (first paragraph in each Wikipedia entry).* Maybe not?"
      ],
      "metadata": {
        "id": "72NMS0uaaE-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Extracting the IT articles from the Category"
      ],
      "metadata": {
        "id": "W2b1tIpecTlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles = []\n",
        "page_texts_it = []\n",
        "\n",
        "def get_it_articles(category, page_titles, page_texts_it):\n",
        "\n",
        "  wiki_wiki = wikipediaapi.Wikipedia('it')\n",
        "  cat = wiki_wiki.page(category)   # category should be like .page(\"Categoria:Survival_horror\")\n",
        "\n",
        "  for p in cat.categorymembers.values():\n",
        "    if p.namespace == wikipediaapi.Namespace.MAIN:\n",
        "      # it is page => we can get text\n",
        "      page_titles.append(p.title)\n",
        "      page_texts_it.append(p.text)"
      ],
      "metadata": {
        "id": "1PzyPtMObURS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_it_articles('Categoria:Videogiochi_in_realt√†_virtuale', page_titles, page_texts_it)"
      ],
      "metadata": {
        "id": "4D89ZsPPb5tU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Extracting the corresponding EN articles"
      ],
      "metadata": {
        "id": "bKiJqtQlcXBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By leveraging the previously obtained titles, we can search for the equivalent pages on the EN Wikipedia."
      ],
      "metadata": {
        "id": "EDlC3ngacbgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_texts_en = []\n",
        "\n",
        "def get_langlinks(page, page_texts_en):\n",
        "\n",
        "        langlinks = page.langlinks\n",
        "\n",
        "        for k in sorted(langlinks.keys()):\n",
        "            v = langlinks[k]\n",
        "\n",
        "        try:\n",
        "          page_en = page.langlinks['en']\n",
        "          page_texts_en.append(page_en.text)\n",
        "\n",
        "        except KeyError:\n",
        "          page_texts_en.append(str('No match'))\n",
        "          \n",
        "        except JSONDecodeError:\n",
        "           page_texts_en.append(str('No match'))"
      ],
      "metadata": {
        "id": "p-a8akWmZFST"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_en_articles(page_titles, page_texts_en):\n",
        "\n",
        "  for title in page_titles:\n",
        "\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('it')\n",
        "    page = wiki_wiki.page(str(title))\n",
        "    get_langlinks(page, page_texts_en)"
      ],
      "metadata": {
        "id": "_OnZucgQbAXQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_en_articles(page_titles, page_texts_en)"
      ],
      "metadata": {
        "id": "hdpRsTX2dDTc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Comparable IT-EN corpus"
      ],
      "metadata": {
        "id": "PfcOkZbBdSdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We merge the IT and the EN pages to obtain a comparable corpus."
      ],
      "metadata": {
        "id": "tFu7FjzBdehY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparable_corpus_vg = pd.DataFrame(np.column_stack([page_texts_en, page_texts_it]), \n",
        "                               columns=['en', 'it'])"
      ],
      "metadata": {
        "id": "-gzx7eLUdXtl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we only need to drop the rows where no English equivalent was found."
      ],
      "metadata": {
        "id": "Ul29Pxtadi62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparable_corpus_vg = comparable_corpus_vg[comparable_corpus_vg['en'].str.contains('No match')==False].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "7xNDNaQJdlol"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End result:"
      ],
      "metadata": {
        "id": "-EAlzKALdsMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparable_corpus_vg"
      ],
      "metadata": {
        "id": "19PNhKjkdtBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extracting parallel sentences from the comparable corpus"
      ],
      "metadata": {
        "id": "hSSoAU_rd9KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fefQrvA9eF8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}